# walmarthw 

Data Pipeline

![Image description](pipeline.jpeg)

The project ingests live tweets using the Twitter Streaming API into a CouchbaseDb. The project is 
setup using the following tech:
1) Java 8
2) Spark 2.2.0
3) Couchbase 6.x
3) Twitter4j - open source repo providing a wrapper over Twitter Streaming API.

The data pipeline as seen in the image has 4 components:
1) Java application - uses the Twitter4j Streaming API to pull live tweets. The application writes it to a 
time based rolling file appender. The application creates a files for each min and puts its in a location 
specified with -Dlog.logBase JVM parameter. If -Dlog.logBase is not specified, the logs will be written to 
./logs/ director. The application also acts as a monitor, keeping count of number tweets received over http, 
number of unique tweets stored in CB and number of tweets received in a session.

2) 1 minute log files that are generated by the java application and are consumed by a spark streaming application.

3) Spark Streaming Application - Spark Structured Streaming application that consumes the 1 min log files
filters outs tweets related to music using basic filter function, repartitions the stream based on tweet id
so ensure duplicates are handled by same executor, theoratically. The transformed tweets are pushed into
couchbase. These mutations in couchbase as upserts to ensure duplicates are not added.

4) Couchbase store - holds tweet documents. The tweet id is used as the document id.

The data pipeline is split up into 2 working process.
1) Java Ingest app, to pull tweets and write to log files.
2) Spark Structured Streaming app to read these log files and push to couchbase.
Splitting up the data pipeline into such component will make it ease to monitor and maintain each component
separately without disturbing the other.

Build Process:
mvn clean package

Before running following must be setup:
1) Twitter dev account. Get consumer key, consumer secret, access token and access secret from twitter
site. These keys/tokens are to be used while running the data pipeline.
2)  A couchbase instance setup locally. 
3) Create a new bucket "twitter" in the local couchbase instance.

NOTE: THE PIPELINE IS SETUP TO RUNNING LOCALLY AS OF NOW.

To run the data pipeline components follow the steps below. RUN THE BOTH APPLCIATION FROM SAME DIR:
1) cd to the root directory of the project.
2) First run the java application:

java -Dcb.username=$CB_USERNAME -Dcb.passwd=$CB_PASSWD -Dtwitter.consumerKey=$TWITTER_CONSUMER_KEY \  
-Dtwitter.consumerSecret=$TWITTER_CONSUMER_SECRET \
-Dtwitter.accessToken=$TWITTER_ACCESS_TOKEN \
-Dtwitter.accessSecret=$TWITTER_ACCESS_SECRET \ 
-cp ./target/walmarthw-1.0-SNAPSHOT.jar com.mpeshave.walmarthw.TwitterStreamingClient

The java application will start writing files to ./log/ directory. The java app will start printing
counts of tweets consumed.

3) Let the java application run for a couple of mins as the spark application will start looking for log files.
4) Open a second terminal to run spark application:
spark-submit --master local[2] --driver-memory 4g \ 
--conf 'spark.driver.extraJavaOptions=-Dcb.username=$CB_USERNAME -Dcb.passwd=$CB_PASSWD' \
--class com.mpeshave.walmarthw.TwitterStreamingIngest ./target/walmarthw-1.0-SNAPSHOT.jar

If everything is setup correctly, documents will start flowing into local couchbase instance.


Questions:

1) What are the risks involved in building such a pipeline?

Firstly, as such a pipeline is based on an external dependecy, the external resource may change without
notification which may cause failures of the ingestion pipeline or receive inconsitent/corrupt data.
Secondly, setting up the pipeline as single process program wouldnt be efficient. So, it will need to be 
split up into multiple component. Having multiple components running in union proves another challenge,
as all the components must be monitored and maintained. Testing such a pipeline end to end is also cumbersome
and requires validations and checks at multiple places.  

2) How would you roll out the pipeline going from proof-of-concept to a production-ready solution?

As the pipeline is split up into multiple components, each components can worked on independently to be pushed
to prod. To start with, the first ingestion component(ingest - consuming the data from twitter) must be worked on.
The component will need some sort of application monitoring and alerts setup. Also, currently the 1 min log files
are written to local disk. To productionalise these will need to be moved to some distributed storage. This 
may require changes to either the application or a separate process move the files to distributed storage. This would
be a good point to push to pre-prod and do some testing and push the ingestion component to prod.
A prod couchbase cluster setup would follow next with monitoring and alerting setup around it. Next, would be to
make changes to the spark streaming application to make it consume files created/stored in cloud storage and changes
to be able to connect to the new couchbase cluster. As this will be a long running streaming application next would 
be to setup monitoring around the nodes/cluster this spark application runs on. As each component is added one at a 
time, integration testing will be needed as well as data validation. To start with these processes can be manual 
but eventually these can be automated. If this is to be moved to cloud, setting up process 
networking/permissions/roles and required infrastructure will also need to be considered.

What would a production-ready solution entail that a POC wouldn't?
1) Requirements on what transformation/cleanup on the data are needed before storing it. 
2) Standard file storage format(avro parquet) for log files, that can handle changing schema and compression.
3) Scalable ingestion pipeline based on load.
4) Better document compression and storage in couchbase. Changed document schema that represents information in a
better way.
5) Checks and validations at various stages of the pipeline.

What is the level of effort required to deliver each phase of the solution?
Assuming level of effort in sprints, each phase will take up to sprint and half to two sprints with 2 
engineers working.

What is your estimated timeline for delivery for a production-ready solution?
First iteration of a production ready solution for this may take up to 4 or 5 sprints.





 
 